{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Lenny Pelhate, Etienne SULTAN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CentraleSupelec - Natural language processing\n",
    "\n",
    "## Natural Language Inferencing (NLI): \n",
    "\n",
    "(NLI) is a classical NLP (Natural Language Processing) problem that involves taking two sentences (the premise and the hypothesis ), and deciding how they are related (if the premise *entails* the hypothesis, *contradicts* it, or *neither*).\n",
    "\n",
    "Ex: \n",
    "\n",
    "\n",
    "| Premise | Label | Hypothesis |\n",
    "| --- | --- | --- |\n",
    "| A man inspects the uniform of a figure in some East Asian country. | contradiction | The man is sleeping. |\n",
    "| An older and younger man smiling. | neutral | Two men are smiling and laughing at the cats playing on the floor. |\n",
    "| A soccer game with multiple males playing. | entailment | Some men are playing a sport. |\n",
    "\n",
    "### Stanford NLI (SNLI) corpus\n",
    "\n",
    "In this labwork, I propose to use the Stanford NLI (SNLI) corpus ( https://nlp.stanford.edu/projects/snli/ ), available in the *Datasets* library by Huggingface.\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    snli = load_dataset(\"snli\")\n",
    "    #Removing sentence pairs with no label (-1)\n",
    "    snli = snli.filter(lambda example: example['label'] != -1) \n",
    "\n",
    "## Subject\n",
    "\n",
    "You are asked to provide an operational Jupyter notebook that performs the task of NLI. For that, you need to tackle the following aspects of the problem:\n",
    "\n",
    "1. Loading and preprocessing the data\n",
    "2. Designing a PyTorch model that, given two sentences, decides how they are related (*entails*, *contradicts* or *neither*.)\n",
    "3. Training and evaluating the model using appropriate metrics\n",
    "4. (Optional) Allowing to play with the model (forward user sentences and visualize the prediction easily)\n",
    "5. (Optional) Providing visual insight about the model (i.e. visualizing the attention if your model is using attention)\n",
    "\n",
    "You can choose between a trained approach (for which I suggest using the huggingface *transformer* library) or a zero-shot or few-shot approach (for which I suggest using a local *ollama* server). You can, of course, do both and compare your results.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation will be based on several criteria:\n",
    "\n",
    "- Clarity and readability of the notebook. The notebook is the report of you project. Make it easy and pleasant to read.\n",
    "- Justification of implementation choices (i.e. the network, the cost funtion, the optimizer, ...)\n",
    "- Quality of the code. The various deeplearning and NLP labworks provide many example of good practices for designing experiments with neural networks. Use them as inspirational examples!\n",
    "\n",
    "## Additional recommendations\n",
    "\n",
    "- You are not seeking to publish a research paper! I'm not expecting state-of-the-art results! The idea of this labwork is to assess that you have integrated the skills necessary to handle textual data using deep neural network techniques.\n",
    "\n",
    "- This labwork will be evaluated but we are still here to help you! Don't hesitate to request our help if you are stuck.\n",
    "\n",
    "- If you intend to use BERT based models, let me give you an advice. The bert-base-* models available in *Transformers* need more than 12Go to be fine-tuned on GPU. To avoid memory issues, you can use several solutions: \n",
    "\n",
    "    - Use a lighter BERT based model such as DistilBERT, ALBERT, ...\n",
    "    - Train a classification model on top of BERT, whithout fine-tuning it (i.e. freezing BERT weights)\n",
    "\n",
    "## Huggingface documentations\n",
    "\n",
    "In case you want to use the huggingface *Datasets* and *Transformer* libraries (which I advice), here are some useful documentation pages:\n",
    "\n",
    "- Dataset quick tour\n",
    "\n",
    "    https://huggingface.co/docs/datasets/quicktour.html\n",
    "    \n",
    "- Documentation on data preprocessing for transformers\n",
    "\n",
    "    https://huggingface.co/transformers/preprocessing.html\n",
    "    \n",
    "- Transformer Quick tour (with distilbert example for classification).\n",
    "\n",
    "    https://huggingface.co/transformers/quicktour.html\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Local models via Ollama\n",
    "\n",
    "### Starting ollama server\n",
    "\n",
    "Open a terminal and run the following command:\n",
    "\n",
    "> mkdir ollama <br>\n",
    "> cd ollama <br\n",
    "> curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz <br>\n",
    "> tar -xzf ollama-linux-amd64.tgz <br>\n",
    "> cd bin/ <br>\n",
    "> ./ollama serve\n",
    "\n",
    "This will start an ollama server accessible at http://localhost:11434\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an assistant for question-answering tasks. Answer the question according only to the given context.\n",
    "If question cannot be answered using the context, simply say I don't know. Do not make stuff up.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "#context = \"Barack Hussein Obama II (born August 4, 1961) is an American politician who was the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African-American president. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004.\"\n",
    "context = \"Barack Hussein Obama II (born August 32, 1861) is an American politician who was the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African-American president. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004.\"\n",
    "question = \"When Barack obama was born ?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (1.64.1)\n",
      "Requirement already satisfied: aiohttp in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (3.11.13)\n",
      "Requirement already satisfied: click in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (8.1.8)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (0.27.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.66.1 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (1.69.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (2.10.6)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (1.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (0.9.0)\n",
      "Requirement already satisfied: tokenizers in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from litellm) (0.21.1)\n",
      "Requirement already satisfied: anyio in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from httpx>=0.23.0->litellm) (4.6.2)\n",
      "Requirement already satisfied: certifi in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from httpx>=0.23.0->litellm) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from httpx>=0.23.0->litellm) (1.0.2)\n",
      "Requirement already satisfied: idna in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from httpx>=0.23.0->litellm) (3.7)\n",
      "Requirement already satisfied: sniffio in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from httpx>=0.23.0->litellm) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.22.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from openai>=1.66.1->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from openai>=1.66.1->litellm) (0.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from openai>=1.66.1->litellm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from openai>=1.66.1->litellm) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from aiohttp->litellm) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from aiohttp->litellm) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from aiohttp->litellm) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from aiohttp->litellm) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from aiohttp->litellm) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from aiohttp->litellm) (1.18.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from tokenizers->litellm) (0.29.1)\n",
      "Requirement already satisfied: filelock in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "August 32, 1861"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "\n",
    "response = completion(\n",
    "  model=\"ollama/mistrallite\",\n",
    "  messages=[{\"content\": system_prompt.format(context=context),\"role\": \"system\"}, {\"content\": user_prompt.format(question=question),\"role\": \"user\"}],\n",
    "  api_base=\"http://localhost:11434\",\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proposition de réponse:**\n",
    "\n",
    "### **Chargement du dataset SNLI:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers if not present\n",
    "# !pip install transformers evaluate datasets accelerate\n",
    "# !pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/sdim/sdim_36/.conda/envs/nlp-tp1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since snli couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /usr/users/sdim/sdim_36/.cache/huggingface/datasets/snli/plain_text/0.0.0/cdb5c3d5eed6ead6e5a341c8e56e669bb666725b (last modified on Tue Mar 25 18:02:11 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "snli = load_dataset(\"snli\")\n",
    "#Removing sentence pairs with no label (-1)\n",
    "snli = snli.filter(lambda example: example['label'] != -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploration du dataset:**\n",
    "\n",
    "**Dataset type:**\n",
    "- 'datasets.dataset_dict.DatasetDict'\n",
    "\n",
    "The dataset is already split into train-test-validation. \n",
    "\n",
    "**Labels:**\n",
    "- Entailment: 0\n",
    "- Neutral: 1\n",
    "- Contradiction: 2\n",
    "\n",
    "On visualise quelques exemples du dataset train :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise:  A person on a horse jumps over a broken down airplane.\n",
      "Hypothesis:  A person is training his horse for a competition.\n",
      "Label:  1 \n",
      "\n",
      "Premise:  A person on a horse jumps over a broken down airplane.\n",
      "Hypothesis:  A person is at a diner, ordering an omelette.\n",
      "Label:  2 \n",
      "\n",
      "Premise:  A person on a horse jumps over a broken down airplane.\n",
      "Hypothesis:  A person is outdoors, on a horse.\n",
      "Label:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('Premise: ', snli[\"train\"][i]['premise'])\n",
    "    print('Hypothesis: ', snli[\"train\"][i]['hypothesis'])\n",
    "    print('Label: ', snli[\"train\"][i]['label'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entailment      (0) : 3368 exemples (34.28%)\n",
      "neutral         (1) : 3219 exemples (32.77%)\n",
      "contradiction   (2) : 3237 exemples (32.95%)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "label_map = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "\n",
    "total = len(snli[\"test\"])\n",
    "test_labels = [example[\"label\"] for example in snli[\"test\"]]\n",
    "label_counts = Counter(test_labels)\n",
    "\n",
    "for label_id in sorted(label_map):\n",
    "    count = label_counts[label_id]\n",
    "    percent = count / total * 100\n",
    "    print(f\"{label_map[label_id]:<15} ({label_id}) : {count} exemples ({percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le  dataset test est équilibré. Ainsi un modèle qui prédirait uniformément une des trois classes aurait une accuracy de presque $1/3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama server\n",
    "\n",
    "Dans cette partie on s'intéresse à l'étude de deux architecture Zero shot et Few Shot model.\n",
    "Pour cela on utilise le modèle mistrallite ainsi qu'un Ollama serveur afin de pouvoir l'utiliser et interagir avec lui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot modèle\n",
    "\n",
    "Le modèle zero shot est un modèle où l'on utilise un modèle de langage qui doit prédire une classe : entailment, neutral ou contradiction. \n",
    "\n",
    "Le modèle ne reçoit qu'un prompt et n'as pas vu d'exemple de la tâche qu'il doit faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "#Pour contraindre le modèle au format on lui dit de renvoyer un JSON\n",
    "def extract_json(content):\n",
    "    match = re.search(r'\\{[^}]+\\}', content)\n",
    "    if match:\n",
    "        json_str = match.group()\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            answer = data.get(\"answer\", \"\").strip().lower()\n",
    "            if answer in {\"entailment\", \"contradiction\", \"neutral\"}:\n",
    "                return answer\n",
    "            else:\n",
    "                return \"invalid_answer\"\n",
    "        except json.JSONDecodeError:\n",
    "            return \"json_error\"\n",
    "    else:\n",
    "        return \"no_json_found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nli_zeroshot(premise, hypothesis):\n",
    "    system_prompt_zeroshot = (\n",
    "        \"Tu es un expert en inférence textuelle. \"\n",
    "        \"Classifie la relation entre deux phrases en répondant uniquement par un objet JSON valide, et rien d'autre. \"\n",
    "        \"Le format doit être : {\\\"answer\\\": \\\"<label>\\\"} où <label> est l'une des options suivantes : entailment, contradiction, neutral. \"\n",
    "        \"Ne rajoute aucune explication.\\n\\n\"\n",
    "    )\n",
    "    user_prompt_zeroshot = (\n",
    "        f\"\"\"\n",
    "        Premise: {premise}\\n\n",
    "        Hypothesis: {hypothesis}\\n\n",
    "        Réponse:\n",
    "        \"\"\"\n",
    "        )\n",
    "    response = completion(\n",
    "        model=\"ollama/mistrallite\",  # ou le nom de modèle que tu utilises\n",
    "        messages=[{\"content\": system_prompt_zeroshot,\"role\": \"system\"}, {\"content\": user_prompt_zeroshot,\"role\": \"user\"}],\n",
    "        api_base=\"http://localhost:11434\",\n",
    "        stream=False\n",
    "    )\n",
    "    # Récupère le contenu de la réponse\n",
    "    content = response.choices[0].message[\"content\"].strip().lower()\n",
    "    return extract_json(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_zero_shot(n_examples=50):\n",
    "    # Pour tester des cas de deboggage, on prend n exemples aléatoires du split \"test\"\n",
    "    # On peut prendre la longueur du dataset test pour évaluer sur tout le dataset test\n",
    "    test_examples = random.sample(list(snli[\"test\"]), n_examples)\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Mapping du dataset SNLI :\n",
    "    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "    for example in test_examples:\n",
    "        try:\n",
    "            premise = example[\"premise\"]\n",
    "            hypothesis = example[\"hypothesis\"]\n",
    "            numeric_label = example[\"label\"]\n",
    "            true_label = label_map[numeric_label]\n",
    "        except KeyError as e:\n",
    "            print(f\"Clé manquante dans l'exemple {example}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement de l'exemple {example}: {e}\")\n",
    "            continue\n",
    "\n",
    "        prediction = predict_nli_zeroshot(premise, hypothesis)\n",
    "        true_labels.append(true_label)\n",
    "        predicted_labels.append(prediction)\n",
    "\n",
    "    # Filtrer les prédictions valides pour le calcul de l'accuracy\n",
    "    valid_pairs = [(t, p) for t, p in zip(true_labels, predicted_labels)\n",
    "                  if p in {\"entailment\", \"contradiction\", \"neutral\"}]\n",
    "\n",
    "    if valid_pairs:\n",
    "        filtered_true, filtered_pred = zip(*valid_pairs)\n",
    "        accuracy = accuracy_score(filtered_true, filtered_pred)\n",
    "        print(f\"Accuracy sur {len(valid_pairs)}/{len(true_labels)} exemples valides: {accuracy:.2f}\")\n",
    "        return accuracy\n",
    "    else:\n",
    "        print(\"Aucune prédiction valide pour calculer l'accuracy.\")\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affecte la variable size à la taille du dataset test\n",
    "size = len(snli[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy sur 8680/9824 exemples valides: 0.37\n",
      "accuracy_few_zero_shot :  0.3721198156682028\n"
     ]
    }
   ],
   "source": [
    "# Calcul de l'accuracy\n",
    "accuracy_few_zero_shot = compute_accuracy_zero_shot(size)\n",
    "print(\"accuracy_few_zero_shot : \",accuracy_few_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Ollama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nli_few_shot(premise, hypothesis):\n",
    "    system_prompt_few_shot = (\n",
    "        \"Your task is to classify the relationship between a premise and a hypothesis as 'entailment', 'contradiction', or 'neutral'.\\n\"\n",
    "        \"You must respond with a valid JSON object, and nothing else.\\n\"\n",
    "        \"Format: {\\\"answer\\\": \\\"<label>\\\"} where <label> is one of: entailment, contradiction, neutral.\\n\"\n",
    "        \"DO NOT add explanations. DO NOT write anything else.\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"Premise: A man is playing guitar.\\n\"\n",
    "        \"Hypothesis: A person is making music.\\n\"\n",
    "        \"Answer: {\\\"answer\\\": \\\"entailment\\\"}\\n\\n\"\n",
    "        \"Premise: A cat is sleeping on a couch.\\n\"\n",
    "        \"Hypothesis: A cat is jumping from a table.\\n\"\n",
    "        \"Answer: {\\\"answer\\\": \\\"contradiction\\\"}\\n\\n\"\n",
    "        \"Premise: A group of people is walking in a park.\\n\"\n",
    "        \"Hypothesis: The group is exercising.\\n\"\n",
    "        \"Answer: {\\\"answer\\\": \\\"neutral\\\"}\\n\\n\"\n",
    "        \"Now classify the following:\\n\"\n",
    "    )\n",
    "    user_prompt_few_shot = (\n",
    "        f\"\"\"\n",
    "        Premise: {premise}\\n\n",
    "        Hypothesis: {hypothesis}\\n\n",
    "        \"\"\"\n",
    "        )\n",
    "    response = completion(\n",
    "        model=\"ollama/mistrallite\",\n",
    "        messages=[{\"content\": system_prompt_few_shot,\"role\": \"system\"}, {\"content\": user_prompt_few_shot,\"role\": \"user\"}],\n",
    "        api_base=\"http://localhost:11434\",\n",
    "        stream=False\n",
    "    )\n",
    "    # Récupère le contenu de la réponse\n",
    "    content = response.choices[0].message[\"content\"].strip().lower()\n",
    "    return extract_json(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_few_shot(n_examples=100):\n",
    "\n",
    "    # Pour tester des cas de deboggage, on prend n exemples aléatoires du split \"test\"\n",
    "    # On peut prendre la longueur du dataset test pour évaluer sur tout le dataset test\n",
    "\n",
    "    test_examples = random.sample(list(snli[\"test\"]), n_examples)\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Mapping du dataset SNLI :\n",
    "    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "    for example in test_examples:\n",
    "        try:\n",
    "            premise = example[\"premise\"]\n",
    "            hypothesis = example[\"hypothesis\"]\n",
    "            numeric_label = example[\"label\"]\n",
    "            true_label = label_map[numeric_label]\n",
    "        except KeyError as e:\n",
    "            print(f\"Clé manquante dans l'exemple {example}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement de l'exemple {example}: {e}\")\n",
    "            continue\n",
    "\n",
    "        prediction = predict_nli_few_shot(premise, hypothesis)\n",
    "        true_labels.append(true_label)\n",
    "        predicted_labels.append(prediction)\n",
    "\n",
    "    # Filtrer les prédictions valides pour le calcul de l'accuracy\n",
    "    valid_pairs = [(t, p) for t, p in zip(true_labels, predicted_labels)\n",
    "                  if p in {\"entailment\", \"contradiction\", \"neutral\"}]\n",
    "\n",
    "    if valid_pairs:\n",
    "        filtered_true, filtered_pred = zip(*valid_pairs)\n",
    "        accuracy = accuracy_score(filtered_true, filtered_pred)\n",
    "        print(f\"Accuracy sur {len(valid_pairs)}/{len(true_labels)} exemples valides: {accuracy:.2f}\")\n",
    "        return accuracy\n",
    "    else:\n",
    "        print(\"Aucune prédiction valide pour calculer l'accuracy.\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy sur 9109/9824 exemples valides: 0.42\n",
      "Accuracy Few shot: 0.42364694258425734\n"
     ]
    }
   ],
   "source": [
    "# Calcul de l'accuracy (à adapter selon tes besoins)\n",
    "acc_few_shots = compute_accuracy_few_shot(size)\n",
    "print(\"Accuracy Few shot:\", acc_few_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot vs Few Shot\n",
    "\n",
    "Dans le cadre de notre tâche de reconnaissance d'inférence textuelle, nous avons comparé deux approches : **Zero Shot** et **Few Shot**.\n",
    "\n",
    "- **Zero Shot** : le modèle n’a accès à aucun exemple spécifique de la tâche. Il s’appuie uniquement sur sa compréhension générale du langage.\n",
    "- **Few Shot** : quelques exemples annotés sont fournis au modèle pour l’aider à mieux comprendre la tâche à accomplir.\n",
    "\n",
    "### Résultats obtenus :\n",
    "\n",
    "| Méthode     | Accuracy (%) | Gain vs. aléatoire (%) |\n",
    "|-------------|--------------|--------------------|\n",
    "| Aléatoire   | 33           | -                  |\n",
    "| Zero Shot   | 37           | +4                 |\n",
    "| Few Shot    | 42           | +9                 |\n",
    "\n",
    "On observe que :\n",
    "- L’approche **Zero Shot** atteint une accuracy de **37%**, légèrement supérieure à la prédiction aléatoire (**33%**).\n",
    "- L’approche **Few Shot** obtient une accuracy de **42%**, ce qui montre un gain plus significatif.\n",
    "\n",
    "Ainsi, même avec très peu d’exemples, le **Few Shot learning** permet d’améliorer les performances du modèle sur une tâche spécifique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie on étudie une architecture **BERT**.\n",
    "\n",
    "Il s'agit de finetuner un modèle pré-entrainé disponible sur HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenisation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(sentence):\n",
    "    tokenized = tokenizer(sentence[\"premise\"], sentence[\"hypothesis\"], padding=\"max_length\", max_length=128, truncation=True) # truncation=True will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model\n",
    "    # Add labels back to the tokenized dictionary\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9842/9842 [00:01<00:00, 9751.76 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Encode dataset\n",
    "snli_encoded = snli.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example {'input_ids': [[101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 1037, 2711, 2003, 2731, 2010, 3586, 2005, 1037, 2971, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 1037, 2711, 2003, 2012, 1037, 15736, 1010, 13063, 2019, 18168, 12260, 4674, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 1037, 2711, 2003, 19350, 1010, 2006, 1037, 3586, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2336, 5629, 1998, 12015, 2012, 4950, 102, 2027, 2024, 5629, 2012, 2037, 3008, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2336, 5629, 1998, 12015, 2012, 4950, 102, 2045, 2024, 2336, 2556, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "example = tokenize_function(snli[\"train\"][:5])\n",
    "print(\"example\", example)\n",
    "print(type(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and eval datasets\n",
    "train_data = snli_encoded[\"train\"]\n",
    "eval_data = snli_encoded[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implémentation de la métrique d'évaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entraînement du modèle:**\n",
    "\n",
    "Nous utilisons DistilBERT comme modèle de base. \n",
    "\n",
    "Nous avons décidé de geler tous les poids de notre DistilBERT sauf le dernier:\n",
    "- Entraîner uniquement la tête du classificateur réduit considérablement le coût computationnel et l'utilisation de la mémoire, rendant l'affinage plus efficace. Cependant, cette méthode limite la capacité du modèle à s’adapter aux nouvelles données, ce qui limitait les performances de notre modèle (accuracy d'environ 55%)\n",
    "- En dégelant la dernière couche du transformeur, on permet une certaine adaptation à la tâche d'nli tout en maintenant un entraînement efficace, trouvant ainsi un équilibre entre économies de calcul et flexibilité du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "# Freeze the DistilBERT base model weights so only the classifier head is trained\n",
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last transformer layer\n",
    "for param in model.distilbert.transformer.layer[-1].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choix des paramètres:**\n",
    "\n",
    "- <i>metric_name = \"accuracy\"</i> :\n",
    "    - Nous avons choisi d'utiliser la précision comme métrique d’évaluation, car elle est simple et efficace pour les tâches de classification.\n",
    "- <i>batch_size = 128</i> :\n",
    "    - Nous avons choisi un grand batch_size pour accélérer l'entraînement.\n",
    "    - Les GPUs utilisés pour faire tourner le modèle ont une capacité suffisante pour des batch de 128, alors que pour 256 par exemple, nous avions des erreurs de mémoire.  \n",
    "- <i>eval_strategy=\"steps\"</i> :\n",
    "    - Afin de surveiller les performances plus fréquemment pendant l’entraînement.\n",
    "- <i>save_strategy=\"steps\"</i> :\n",
    "    - Sauvegarde du modèle toutes les 250 étapes pour garantir que le meilleur modèle soit conservé au fur et à mesure de l'entraînement.  \n",
    "- <i>learning_rate=5e-5</i> :\n",
    "    - Choix d'un taux d'apprentissage légèrement plus élevé que la norme. Nous avions testé plusieurs \"learning rates\", entre [1e-5, 5e-4]\n",
    "    - Ce learning rate semble adapté (au vu de nos tests) à un modèle dont seule la dernière couche du transformeur est fine-tunée et dont le reste des poids est gelé.\n",
    "- <i>num_train_epochs=3</i> :\n",
    "    - Nous avons fixé le nombre d'époques à 3, il s'agit d'un compromis entre avoir un entraînement suffisant pour la tâche d'nli et avoir un temps d'entraînement pas trop long.  \n",
    "- <i>weight_decay=0.001</i> :\n",
    "    - Nous avons choisi de faire une régularisation L2 (weight decay) pour éviter le surapprentissage en pénalisant les poids trop grands du modèle.  \n",
    "- <i>warmup_steps=int(0.05*len(train_data)/batch_size)</I> :\n",
    "    - C'est une pratique courante d’utiliser 5-10% des étapes totales pour l’échauffement, permettant au modèle de stabiliser l'apprentissage en début d'entraînement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-SNLI5\",\n",
    "    eval_strategy= \"steps\", # monitor performance more frequently\n",
    "    eval_steps=250,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=250,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    #weight_decay=0.001, # L2 regularization the optimizer to prevent overfitting by penalizing large weights in the model\n",
    "    warmup_steps=int(0.05*len(train_data)/batch_size), # common practice to use 5-10% of total steps for warmup (5% since frozen weights)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    lr_scheduler_type=\"reduce_lr_on_plateau\",  # Reduce learning rate when performance plateaus\n",
    "    # fp16=True,  # Enable mixed precision training for faster training\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/sdim_32-105091/ipykernel_4023666/3162535751.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12876' max='12876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12876/12876 38:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696240</td>\n",
       "      <td>0.780837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.380500</td>\n",
       "      <td>0.521546</td>\n",
       "      <td>0.799837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.380500</td>\n",
       "      <td>0.499915</td>\n",
       "      <td>0.803394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>0.483518</td>\n",
       "      <td>0.803698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>0.475946</td>\n",
       "      <td>0.810404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>0.473947</td>\n",
       "      <td>0.814062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>0.472094</td>\n",
       "      <td>0.811928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.471850</td>\n",
       "      <td>0.812335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.462318</td>\n",
       "      <td>0.817618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.469238</td>\n",
       "      <td>0.817923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.459992</td>\n",
       "      <td>0.820667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>0.457755</td>\n",
       "      <td>0.824426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>0.446915</td>\n",
       "      <td>0.826661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.515800</td>\n",
       "      <td>0.443644</td>\n",
       "      <td>0.826255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.515800</td>\n",
       "      <td>0.439940</td>\n",
       "      <td>0.829506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.509700</td>\n",
       "      <td>0.438432</td>\n",
       "      <td>0.829608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.509700</td>\n",
       "      <td>0.438758</td>\n",
       "      <td>0.828287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.438518</td>\n",
       "      <td>0.829811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.440106</td>\n",
       "      <td>0.829811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>0.438925</td>\n",
       "      <td>0.828287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>0.437732</td>\n",
       "      <td>0.831030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.438127</td>\n",
       "      <td>0.829811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.436693</td>\n",
       "      <td>0.831132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.436538</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.436317</td>\n",
       "      <td>0.830725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.436023</td>\n",
       "      <td>0.830217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.435751</td>\n",
       "      <td>0.830827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.467300</td>\n",
       "      <td>0.435728</td>\n",
       "      <td>0.830522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.467300</td>\n",
       "      <td>0.435651</td>\n",
       "      <td>0.831030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>0.435156</td>\n",
       "      <td>0.831233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>0.435207</td>\n",
       "      <td>0.831538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>0.435069</td>\n",
       "      <td>0.831437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>0.435056</td>\n",
       "      <td>0.831030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>0.435052</td>\n",
       "      <td>0.831742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>0.435070</td>\n",
       "      <td>0.831640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.435102</td>\n",
       "      <td>0.831742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.435075</td>\n",
       "      <td>0.831843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.464500</td>\n",
       "      <td>0.435044</td>\n",
       "      <td>0.831843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>0.464500</td>\n",
       "      <td>0.435014</td>\n",
       "      <td>0.831437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.466500</td>\n",
       "      <td>0.434968</td>\n",
       "      <td>0.831538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>0.466500</td>\n",
       "      <td>0.434957</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.434974</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.434968</td>\n",
       "      <td>0.831437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.457700</td>\n",
       "      <td>0.434957</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>0.457700</td>\n",
       "      <td>0.434990</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>0.434988</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>0.434988</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.434984</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.434988</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.434990</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.434988</td>\n",
       "      <td>0.831335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"HomeDistilBert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test du modèle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"HomeDistilBert\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Entailment       0.85      0.87      0.86      3368\n",
      "Contradiction       0.79      0.80      0.79      3219\n",
      "      Neutral       0.86      0.83      0.84      3237\n",
      "\n",
      "     accuracy                           0.83      9824\n",
      "    macro avg       0.83      0.83      0.83      9824\n",
      " weighted avg       0.83      0.83      0.83      9824\n",
      "\n",
      "\n",
      "Sample Predictions:\n",
      "\n",
      "Example 1:\n",
      "Text: this church choir sings to the masses as they sing joyous songs from the book at a church. the church has cracks in the ceiling.\n",
      "True Label: 1\n",
      "Predicted Label: 2\n",
      "\n",
      "Example 2:\n",
      "Text: this church choir sings to the masses as they sing joyous songs from the book at a church. the church is filled with song.\n",
      "True Label: 0\n",
      "Predicted Label: 0\n",
      "\n",
      "Example 3:\n",
      "Text: this church choir sings to the masses as they sing joyous songs from the book at a church. a choir singing at a baseball game.\n",
      "True Label: 2\n",
      "Predicted Label: 2\n",
      "\n",
      "Example 4:\n",
      "Text: a woman with a green headscarf, blue shirt and a very big grin. the woman is young.\n",
      "True Label: 1\n",
      "Predicted Label: 1\n",
      "\n",
      "Example 5:\n",
      "Text: a woman with a green headscarf, blue shirt and a very big grin. the woman is very happy.\n",
      "True Label: 0\n",
      "Predicted Label: 1\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, snli_encoded, tokenizer):\n",
    "    # Set model to evaluation mode and move it to the appropriate device\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare test dataset\n",
    "    test_dataset = snli_encoded[\"test\"]\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Loop through the test dataset for evaluation\n",
    "    for i in range(len(test_dataset)):\n",
    "        # Convert lists to tensors and add a batch dimension with unsqueeze(0)\n",
    "        input_ids = torch.tensor(test_dataset[i]['input_ids']).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(test_dataset[i]['attention_mask']).unsqueeze(0).to(device)\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "        # Get the model prediction without tracking gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred = outputs.logits.argmax(dim=-1).item()\n",
    "\n",
    "        predictions.append(pred)\n",
    "        true_labels.append(test_dataset[i]['label'])\n",
    "\n",
    "    # Print a detailed classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(\n",
    "        true_labels,\n",
    "        predictions,\n",
    "        target_names=['Entailment', 'Contradiction', 'Neutral']\n",
    "    ))\n",
    "\n",
    "    # Print sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    for i in range(min(5, len(test_dataset))):\n",
    "        input_ids = torch.tensor(test_dataset[i]['input_ids']).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(test_dataset[i]['attention_mask']).unsqueeze(0).to(device)\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred = outputs.logits.argmax(dim=-1).item()\n",
    "\n",
    "        # Decode the input_ids to show the original text (e.g., the premise)\n",
    "        text = tokenizer.decode(test_dataset[i]['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"True Label: {test_dataset[i]['label']}\")\n",
    "        print(f\"Predicted Label: {pred}\")\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_model(model, snli_encoded, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77/77 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: {'eval_loss': 0.42883583903312683, 'eval_accuracy': 0.8326547231270358, 'eval_runtime': 7.8873, 'eval_samples_per_second': 1245.548, 'eval_steps_per_second': 9.763, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_results = trainer.evaluate(eval_dataset=snli_encoded[\"test\"])\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Test results:\", test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pistes d'amélioration:**\n",
    "\n",
    "**Augmenter le nombre d'epochs** :  \n",
    "- Actuellement, le nombre d'époques est fixé à 3, ce qui est souvent suffisant pour une première évaluation rapide. Cependant, il peut être utile d'augmenter le nombre d'époques à 5 ou plus pour permettre au modèle de s'adapter davantage aux données et de mieux exploiter les caractéristiques de l'apprentissage.\n",
    "\n",
    "**Changer le learning rate en fonction des performances** :  \n",
    "- Si la précision ne s'améliore plus après plusieurs évaluations, il peut être utile d'ajuster dynamiquement le learning rate. On peut réduire le learning rate lorsque la performance ne progresse plus, ce qui permet de mieux explorer les minima locaux et d'affiner davantage les poids. \n",
    "\n",
    "**Augmenter le \"warmup steps\"** :  \n",
    "- Si l’on utilise un learning rate faible au début, il peut être utile d'augmenter le nombre de \"warmup steps\" pour permettre une montée en puissance progressive du learning rate au début de l'entraînement. Cela peut aider à éviter des ajustements trop brusques des poids, particulièrement si la convergence est lente. \n",
    "\n",
    "**Utilisation de techniques de régularisation supplémentaires** :  \n",
    "- Afin de prévenir le surapprentissage, nous pourrions explorer d'autres techniques de régularisation comme le dropout, ou d'ajuster davantage le weight decay de notre régularisation actuelle. \n",
    "\n",
    "**Utilisation de batch size plus grand ou plus petit** :  \n",
    "- On pourrait essayer d'augmenter la taille du batch size si la mémoire GPU le permet, pour augmenter la vitesse d'entraînement. Cependant, si le batch size est trop grand, cela peut mener à une moindre capacité de généralisation. Il s'agit donc de trouver un équilibre optimal.\n",
    "\n",
    "**Explorer d'autres modèles** :  \n",
    "- Il peut être pertinent d'essayer des variantes plus grandes comme BERT ou RoBERTa pour voir si elles offrent des gains de performance significatifs.\n",
    "\n",
    "**Améliorer les données d'entrée** :  \n",
    "- Si le modèle ne parvient pas à bien apprendre, il peut être utile d'explorer des méthodes de \"data augmentation\" ou de \"prétraitement\" des données, comme le nettoyage, la normalisation des textes ou l'ajout de bruit pour augmenter la robustesse du modèle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
